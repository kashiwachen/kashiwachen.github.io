# Data Engineer Features

In this article, I mainly summarize the duty and qualification as a __Data Engineer__. 
General data mining process in CRISP-DM模型(**CR**oss-**I**ndustry **S**tandard **P**rocess for **D**ata **M**ining):

- 业务理解 Business Issue Understanding
- 数据理解 Data Understanding
- 数据准备 Data Preparation
- 分析/建模 Analysis/Modeling
- 模型评估 Validation
- 模型发布/可视化 Presentation/Visualization

[toc]

## [Google Data Engineer Certification](https://cloud.google.com/certification/data-engineer)

A Professional Data Engineer enables data-driven decision making by *collecting, transforming, and publishing* data. A Data Engineer should be able to **design, build, operationalize, secure, and monitor data processing systems** with a particular emphasis on security and compliance; scalability and efficiency; reliability and fidelity; and flexibility and portability. A Data Engineer should also be able to leverage, deploy, and continuously train pre-existing machine learning models. [Here](./google_data_engineer_exam_guide.md) is the exam guide.

### Requirement:

- Design data processing systems
- Build and operationalize data processing systems
- Operationalize machine learning models
- Ensure solution quality

----

## Amazon: Data Engineer
We have an exciting opportunity available for a Data Engineer to play a key role in building Analytics Platform that solve business problems in sales organization by data-driven approach. In Sales strategy and Operations team, you will improve sales productivity by providing Analytics Platform and automation tools. This is a great position for someone who is interested in connecting business problem and technology solutions.

You will build **[ETL](https://en.wikipedia.org/wiki/Extract,_transform,_load)(extract, transform, load) process** that consolidate **multiple data source** and **clean up unstructured Data** for Data Analyst and BI Engineer. In addition, you will contribute organization level sales productivity by developing automation tools.
### Duty
- Design, implement, and automate deployment of our distributed system for **collecting and processing log events** from multiple sources
- **Design data schema** and operate internal **data warehouses and SQL/NoSQL database systems**
- Own the design, development, and maintenance of ongoing metrics, reports, analysis, and dashboards to drive key business decisions
- **Monitor and troubleshoot** operational or data issues in the data pipelines
- Drive architectural plans and implementation for future **data storage, reporting, and analytic solutions**
- Work collaboratively with business analysts, data scientists, and other internal partners to identify opportunities/problems
- Provide assistance to the team with troubleshooting, researching the root cause, and thoroughly resolving defects in the event of a problem

### Qualification

- [x] Currently enrolled in or will receive a Bachelor’s or Master’s Degree in math/statistics/engineering or other equivalent quantitative discipline at time of application (or graduated less than six months prior to application)
- [x] Experience with one or more query language (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala)
- [x] Experience with one or more scripting language (e.g., Python, KornShell)
- [ ] Experience with data mining, data warehouse solutions, and ETL

### Preferred Qualification

- [x] Previous technical internship(s)
- [x] Master’s or advance technical degree
- [x] Experience with several **query languages**, schema definition languages, and **scripting languages**
- [x] Experience in **writing and optimizing SQL queries** in a business environment with large-scale, complex datasets
- [ ] Experience with data visualization software (e.g., AWS QuickSight or Tableau) or open-source project
- [ ] Experience with big **data processing** technology (e.g., Hadoop or ApacheSpark), **data warehouse** technical architecture, infrastructure components, ETL, and **reporting/analytic tools** and environments
- [ ] Ability to deal with **ambiguity in a fast-paced environment**

## APPLE: Apple Pay Database Engineer - NoSQL and Cloud

### Duty:

Apple Pay Engineering is looking for a passionate **NoSQL** Engineer to help lead and grow our large scale environments in **Cloud**. This team is responsible for providing **new architectures and scalability solutions** for our ever growing business and data processing needs in Cloud.

This position requires a highly motivated individual who likes large scale challenges in a fast paced environment. The successful candidate has the ability to think out of the box and come up with innovative solutions or architectures to meet business requirements in the cloud. This role requires the ability to work independently on live production systems while collaborating with multiple teams. This will require **managing large scale data, automation, and replication** as a service across different data stores and apps for Apple's critical global financial services.

### Qualification

- [ ] Experience working with the databases in the public cloud, specifically AWS.
- [ ] Ability to configure and tune Cassandra, ElasticSearch, Redis, Kafka and Zookeeper clusters.
- [ ] Experience automating routine DBA tasks, CI/CD Pipeline.
- [ ] Experience with GitHub, Ansible, Terraform, Cloud Formation, Python, Docker.
- [ ] Experience implementing HA/DR/Maintenance/Backup strategies in complex systems.
- [ ] Experience developing monitoring and automation solutions for production systems in the cloud.
- [ ] Collaborate in the evaluation, selection and platform design of emerging database technologies with concentration in the NoSQL and cloud spaces.
- [ ] Good understanding of Chef and experience in database automation using Python, Chef, Ansible.
- [ ] Self-directed fast learner who is generous with their knowledge.
- [ ] Strong skills with Linux operating systems running in production.
- [ ] Proven track record of taking ownership and successfully delivering results.
- [ ] The successful candidate should also be able to demonstrate a high degree of competence in communication, ability to learn new technologies, customer centricity and team building.

----

## Agoda JP

### Duty:

- Build, administer and scale **data processing** pipelines

- Design, build, test and deploy new libraries, frameworks or full systems for our core systems while keeping to the highest standards of testing and code quality
- Improve scalability, stability, accuracy, speed and efficiency of our existing data systems
- Work with experienced engineers and product owners to identify and build tools to automate many large-scale data management / analysis tasks

### Qualification

- [ ] Experience in JVM languages (Java/Scala in particular)
- [ ] A good understanding of __data architecture principles__ preferred
- [ ] Experience debugging and reasoning about production issues is desirable

### Skill Packages

- [x] Scala, Golang, **Python3**, **scripting** (Bash/Python)
- [ ] Good understanding of Hadoop ecosystems
- [ ] Working in an agile environment using **test driven methodologies**

----

## SmartNews: Software Engineer, Backend

### Duty

- Develop on server side for news or ads feeds
- Continuous improvement on the system operation, stability, efficiency and scalability

### Qualifications

- [ ] Advanced skill and experience in more than one programming language
- [x] Understanding of computer science
- [ ] Possible to use Japanese or English as fluent or native speaker.

### Preferred Qualifications

- [x] High programming skills in one of these languages: Java, Scala, Kotlin, Python, Ruby
- [ ] Failure analysis and performance tuning in JVM operation
- [x] Development experience on AWS
- [ ] Development and operation experience in high traffic web service
- [ ] Configuration, development and operation of Microservice / Service Mesh architecture
- [ ] Experience in SpringBoot, **MySQL**, PipelineDB, Elasticsearch, Hive, Hadoop, Spark, **Docker**, Kubernetes, Consul, Istio etc.

----

## Line: Solution Engineer,  Data Platform室, Data Engineering Center

LINEまたはLINE関連サービスを通じて生成されるデータの収集、活用を実践するためのプラットフォームやインフラを提供しています。LINEが扱う膨大な規模のデータ処理を実現するため、プラットフォームはOSS等を組み合わせて内製しています。LINEが注力して取り組むデータ活用のコアとなるプラットフォーム開発に、直接貢献することができます。

### Develop env

- [ ] Programming language: Java / Python
- [ ] Develope tools: IntelliJ, Git enterprise, Airflow, Tableau
- [ ] エコシステム: Hive, Presto, Spark,Kuberenetes

### Qualification

### Preferred Qualifications

- [ ] Kafak, FlinkなどのStreaming関連システムの理解

- [ ] Machine Learning関連プラットフォーム構築の経験

- 積極的にコミュニケーションをし、情報共有や問題把握能力を持っている方

- 様々な技術やシステムを手早く把握するための技術的な知識や好奇心をお持ちの方

----

## Rakuten: Entry-Level Engineer (Data Storage and Processing Section) 

### Duty

- Design, implement and maintain Rakuten’s core data storage platform to provide highly scalable, reliable, secure, highly available, and fully optimized data storage solution.
- Design and build high quality but flexible and stable automation pipeline for operations.
- Incident handling and problem-solving.

### Qualification

- [ ] Have enough knowledge to pass one of the certificates below.
  - LPIC level 3
  - LPI DevOps Tools Engineer
  - RHCSA
- [x] Basic skills to read and write at least two of the following language: python / ruby / perl / bash / php / java
- [x] Eagerness to work in a borderless system and human environment.

### Preferred Qualification

- [ ] Knowledge of tStorage system, especially field of NFS, CIFS, SMB, Object Storage. (As a Storage Engineer)
- [ ] Hands-on experience on jenkins.
- [ ] Hands-on experience on configuration management tool.
- [x] Hands-on experience on git.

## Summary

- Transfer unstructed data to structed data for others to analysize and report:
  - Data Lake and Data Warehouse(Storage)
  - Data process(ETL)
    - preprocess monitoring
    - 
- 

